{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5ef990",
   "metadata": {},
   "source": [
    "#### 1. Cite 5 diferenças entre o AdaBoost e o GBM. <br><br>\n",
    "\n",
    "#### 1 Complexidade da árvore\n",
    "Adaboost - Cria stumps, que são um nó com duas folhas.\n",
    "GBM - Cria árvores rasas porém mais complexas.\n",
    "\n",
    "#### 2 Evolução do modelo conforme as iterações.\n",
    "Adaboost - A cada iteração a stump com maior acerto é escolhida para servir de base na criação de um novo conjunto de dados no bootstrap e também fica salva para contribuir com a conclusão junto com as stumps criadas nos loopings posteriores, as stumps tem pesos diferentes de acordo com seu acerto.\n",
    "\n",
    "GBM - A primeira iteração o y predito será a média dos valores, com base nesse valor é calculado o resíduo da média dos valores e posteriormente é criado um novo modelo para fazer predição dos resíduos como se fosse o y, assim temos o resíduo do resíduo e esse resultado é multiplicado pela learning rate, geralmente 0.1, sendo (pred n) + (learning rate * (pred n+1)), repetindo N vezes de acordo com o hyperparametro.\n",
    "\n",
    "#### 3 Peso na decisão\n",
    "\n",
    "Adaboost - Todas stumps contribuem para o resultado final, porém, as stumps tem pesos diferentes conforme sua quantidade de acerto.\n",
    "\n",
    "GBM - As árvores vão fazer uma ponderação do resultado final e o peso de suas respostas vão variar com sua capacidade de redução de resíduos, tendo em vista que toda nova árvore nova é criada para corrigir os erros residuais da árvore anterior.\n",
    "\n",
    "#### 4 Processo de treinamento\n",
    "\n",
    "Adaboost - Ajusta novos modelos com base nos erros dos anteriores, dando maior peso para as linhas que tiveram precição incorreta.\n",
    "\n",
    "GBM - É sequencial como o AdaBoost, porém seus modelos vão focar em melhorar os resíduos anteriores, tendo seu inicio com a média da variável resposta.\n",
    "\n",
    "#### 5 Outliers\n",
    "\n",
    "Adaboost - Tem seu desafio na abordagem de outliers devido seus modelos se basearem nos erros dos modelos anteriores, sendo assim, pode tentar se ajustar demais aos outliers que vão ser os prováveis erros cometidos e comprometer a resposta geral.\n",
    "\n",
    "GBM - O GBM tem o mesmo desáfio que outros algoritimos que usam árvores no geral, se sua complexidade for muito grande, como profundidade por exemplo, ele tende a se ajustar demais aos dados atípicos, mas pode ser melhorado ajustando hyperparametros como profundidade máxima e um learning rate menor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c6a94",
   "metadata": {},
   "source": [
    "#### 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0d884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c919fca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06bea8",
   "metadata": {},
   "source": [
    "#### 3. Cite 5 Hyperparametros importantes no GBM. <br><br>\n",
    "\n",
    "##### n_estimator - Estipula quantas árvores vão ser criadas, ou seja, quantas iterações vão ser feitas até que todo processo seja finalizado, sendo que cada nova árvore vai ser criada levando em consideração os resíduos da anterior. <br><br>\n",
    "\n",
    "##### max_depth - Profundidade máxima das árvores, árvores profundas tendem a se ajustar melhor aos dados, porém também podem aumentar a chance de overfitting.<br><br>\n",
    "\n",
    "##### learning_rate - É o multiplicador da diferença de resíduos para o resultado real. Valores menores diminuem a quantidade que cada árvore contribui para o resultado final, o que melhora a sua gerenalização, porém se faz necessário a utilização de mais árvores para atingir o mesmo desempenho.<br><br>\n",
    "\n",
    "##### subsample - É utilizado para definir quantas features(colunas explicativas) vão ser usadas para tirar as conclusões das árvores, uma quantidade limitada de features usadas pode ajudar a dar mais diversidade nas árvores e ajudar na generalização.<br><br>\n",
    "\n",
    "##### min_samples_leaf - Define quantas amostras devem conter em uma folha para essa folha ser levada em consideração, isso evita que folhas com amostras muito especificas sejam levadas em consideração e melhore a capacidade de generalização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ae3ca",
   "metadata": {},
   "source": [
    "#### 4. (Opcional) Utilize o GridSearch para encontrar osmelhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9790d61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 3, 'max_iter': 150, 'min_samples_leaf': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando dicionário de hyperparametros\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_iter': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Criando classificador GBM\n",
    "clf = HistGradientBoostingClassifier()\n",
    "\n",
    "# Usando grid_search com o GBM, a lista de hyperparametros e 3 folders\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "# Treinando os dados\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo os melhores parametros\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb68bac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hyperparâmetros: {'learning_rate': 0.2, 'max_depth': 3, 'max_iter': 150, 'min_samples_leaf': 1}\n",
      "Acurácia no Conjunto de Teste: 0.9019\n"
     ]
    }
   ],
   "source": [
    "# Criando novo classificador com os melhores parametros\n",
    "best_clf = HistGradientBoostingClassifier(**best_params)\n",
    "# Treinando o modelo com os melhores parametros\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo acuracia do modelo\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Melhores Hyperparâmetros:\", best_params)\n",
    "print(\"Acurácia no Conjunto de Teste:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05834acf",
   "metadata": {},
   "source": [
    "#### 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos? <br><br>\n",
    "\n",
    "##### O SGB introduz aleatoriedade no processo de treinamento, cada árvore usa uma subamostra aleatória dos dados em cada iteração. O hyperparametro subsample controla o tamanho da subamostra utilizada em cada iteração."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
